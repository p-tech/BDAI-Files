{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-tech/BDAI-Files/blob/main/Topic%20Modelling%20-%20Google%20ADs%20-%20Search%20Terms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modelling for review analysis**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a classic model to do topic modelling. Topic modeling is unsupervised learning and the goal is to group different documents to the same “topic”."
      ],
      "metadata": {
        "id": "7vmwZEW9_PUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. import data you would like to analyse**"
      ],
      "metadata": {
        "id": "CCwhAcbc_7NO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLAdb7rY-qUG"
      },
      "outputs": [],
      "source": [
        "# Let's import data first\n",
        "# Run the code and upload the csv file from your laptop\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = 'search-report.csv'\n",
        "df = pd.read_csv(data) # change the csv file name to your file name that you uploaded\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YDl5IA5VMHtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'Search term' #change the column name to where the review is"
      ],
      "metadata": {
        "id": "MIVF0TNWAu9o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Text cleaning & Text processing**\n",
        "Before you do topic modeling, you need make sure you clean and process the text."
      ],
      "metadata": {
        "id": "FwEqMOqvCeKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# drop data with missing values in the 'content' column\n",
        "df = df.dropna(subset=[target_column])\n",
        "\n",
        "# drop duplicate review content\n",
        "df = df.drop_duplicates(subset=[target_column])\n",
        "\n",
        "# remove contraction\n",
        "df[target_column] = df[target_column].map(lambda x: contractions.fix(x))\n",
        "\n",
        "# convert the relevant column to lowercase\n",
        "df[target_column] = df[target_column].str.lower()\n",
        "\n",
        "# Remove overspace\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('\\s{2,}', \" \", x))\n",
        "\n",
        "# Remove non-word characters, so numbers and ___ etc\n",
        "df[target_column] = df[target_column].str.replace(\"[^A-Za-z]\", \" \", regex = True)\n",
        "\n",
        "# Remove punctuation\n",
        "df[target_column] = df[target_column].map(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))"
      ],
      "metadata": {
        "id": "eEnsrDs8A2eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of the contents from the 'contents' column\n",
        "words = df[target_column].tolist()\n",
        "\n",
        "# tokenise the words\n",
        "word_tokens = []\n",
        "for content in words:\n",
        "    word_tokens.append(word_tokenize(content))\n",
        "\n",
        "# create bigram model\n",
        "bigram = gensim.models.phrases.Phrases(word_tokens, min_count=3, threshold=10)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram) # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "\n",
        "# NLTK Stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['coach','trip']) #add more stopwords here\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Define functions for stopwords, bigrams and lemmatisation\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out\n",
        "\n",
        "# Remove stopwords\n",
        "data_words_nostops = remove_stopwords(word_tokens)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Do lemmatisation keeping only noun, adj, vb, adv\n",
        "data_lemmatised = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "# put the tokens back together as text to have our filtered contents\n",
        "\n",
        "rejoin = []\n",
        "for content in data_lemmatised: # Here we choose to use stemming instead of lemmatisation\n",
        "    x = \" \".join(content) # join the text back together\n",
        "    rejoin.append(x)\n",
        "\n",
        "# add the reformed text to the data frame\n",
        "df['cleaned_review'] = rejoin"
      ],
      "metadata": {
        "id": "phOzbFcrMdvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KhEtZvddebij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Build the LDA model**"
      ],
      "metadata": {
        "id": "g_MX7WkJJN-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorise the data into word counts\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "max_words = 1000 #how many words taking account for topic modeling\n",
        "vectorizer = CountVectorizer(max_features=max_words)\n",
        "vec = vectorizer.fit_transform(df['cleaned_review'])\n",
        "\n",
        "k = 3 #this is the number of the topic. you can decide the number\n",
        "\n",
        "lda = LDA(n_components=k, max_iter=5, learning_method='online', random_state = 10)\n",
        "lda.fit(vec)"
      ],
      "metadata": {
        "id": "L0SAqJIzDuba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Visualisation of the topics**"
      ],
      "metadata": {
        "id": "-n28wuX2JXq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "\n",
        "#declaring number of terms we need per topic\n",
        "terms_count = 50\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "wcloud = wordcloud.WordCloud(background_color=\"White\",mask=None, max_words=100,\\\n",
        "                             max_font_size=60,min_font_size=10,prefer_horizontal=0.9,\n",
        "                             contour_width=3,contour_color='Black',colormap='Set2')\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(30, 15), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx,topic in enumerate(lda.components_):\n",
        "    print('Topic# ',idx+1)\n",
        "    abs_topic = abs(topic)\n",
        "    topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]\n",
        "    topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]\n",
        "    topic_words = []\n",
        "    for i in range(terms_count):\n",
        "        topic_words.append(topic_terms_sorted[i][0])\n",
        "    print(','.join( word for word in topic_words))\n",
        "    print(\"\")\n",
        "    dict_word_frequency = {}\n",
        "\n",
        "    for i in range(terms_count):\n",
        "        dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]\n",
        "\n",
        "    ax = axes[idx]\n",
        "    ax.set_title(f'Topic {idx +1}',fontdict={'fontsize': 30})\n",
        "    wcloud.generate_from_frequencies(dict_word_frequency)\n",
        "    ax.imshow(wcloud, interpolation='bilinear')\n",
        "    ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "elXstxIYGzct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise the result into bar charts in topic\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper function to plot topics\n",
        "# see Grisel et al.\n",
        "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(1, 6, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f'Topic {topic_idx +1}',\n",
        "                     fontdict={'fontsize': 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "        for i in 'top right left'.split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "n_top_words = 20  #how many words to be visualised in each topic\n",
        "\n",
        "# get the list of words (feature names)\n",
        "vec_feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# print the top words per topic\n",
        "plot_top_words(lda, vec_feature_names, n_top_words, 'Topics in LDA model')"
      ],
      "metadata": {
        "id": "dKV08bKSEoYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Download file with assigned topic**"
      ],
      "metadata": {
        "id": "UpRn6KhZg1xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "doc_topic = lda.transform(vec)\n",
        "docsVStopics = pd.DataFrame(doc_topic, columns=[\"Topic\"+str(i+1) for i in range(k)])\n",
        "\n",
        "df = df.join(docsVStopics, rsuffix='_topic', lsuffix='_original')\n",
        "#df = df.join(docsVStopics)\n",
        "df['mostlikely_topic'] = docsVStopics.idxmax(axis=1)\n",
        "\n",
        "df.to_csv('topic_modeling_result.csv', index=False) # save the file to google drive\n",
        "files.download('topic_modeling_result.csv') # download the file to your local machine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SXEQdUAVgtmY",
        "outputId": "d6600ea2-9d9a-4318-b6a2-baa75690dc1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_87b989b6-6c72-460c-ba25-47d7825345b2\", \"topic_modeling_result.csv\", 1379070)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}