{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMl9VzyOWn3pNY1CIBs3Ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-tech/BDAI-Files/blob/main/Crawl_SiteMap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMQJ0tYIFH8m"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "# Function to extract URLs from a sitemap\n",
        "def get_urls_from_sitemap(sitemap_url):\n",
        "    response = requests.get(sitemap_url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch sitemap: {response.status_code}\")\n",
        "        return []\n",
        "    # Parse the sitemap\n",
        "    root = ET.fromstring(response.content)\n",
        "    namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "    urls = [url.text for url in root.findall('.//ns:loc', namespaces)]\n",
        "    return urls\n",
        "\n",
        "# Function to crawl a page and extract desired items\n",
        "def crawl_page(url):\n",
        "    proxy = {\n",
        "    }\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {url}: {response.status_code}\")\n",
        "        return None\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Example: Extract title and meta description\n",
        "    title = soup.title.string if soup.title else \"No Title\"\n",
        "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "    meta_desc_content = meta_desc['content'] if meta_desc else \"No Meta Description\"\n",
        "\n",
        "    h1 = soup.find('h1').get_text() if soup.find('h1') else \"\"\n",
        "    h2 = soup.find('h2').get_text()  if soup.find('h2') else \"\"\n",
        "    #price = soup.find('p', class_='property-price').get_text(strip=True)\n",
        "    #price = re.sub(r'[^\\d]', '', price)\n",
        "    #price = re.search(r'\\d[\\d,]*', text).group().replace(',', '')\n",
        "    #element = soup.find('div', class_='property-content')\n",
        "    #if element:\n",
        "    #  text_content = element.get_text(separator='\\n', strip=True)  # Extracts text and removes leading/trailing whitespace\n",
        "    #  print(text_content)\n",
        "    #else:\n",
        "    #  print(\"Element not found\")\n",
        "\n",
        "    return {\"URL\": url, \"Title\": title, \"Meta Description\": meta_desc_content, \"H1\": h1, \"H2\": h2 }\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    sitemap_url = \"https://www.benburys.co.uk/sitemaps/properties.xml\"  # Replace with your sitemap URL\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    data = []\n",
        "    capture = 0\n",
        "\n",
        "    for url in urls:\n",
        "        if capture < 5:\n",
        "          extracted_data = crawl_page(url)\n",
        "          if extracted_data:\n",
        "              data.append(extracted_data)\n",
        "\n",
        "          print(f'extracting data from {url}')\n",
        "\n",
        "        capture = capture + 1\n",
        "\n",
        "\n",
        "    # Save to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    # Save to CSV\n",
        "    df.to_csv('extracted_data.csv', index=False)\n",
        "    print(\"Data saved to extracted_data.csv\")\n",
        "    return df\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    df = main()"
      ]
    }
  ]
}