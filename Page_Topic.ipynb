{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUKpQ5yDwTFO7eYGQBbBJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-tech/BDAI-Files/blob/main/Page_Topic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: add a function to suggest a topic for the page using the keywords even if using ChatGPT to do this\n",
        "\n",
        "import requests\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from nltk.probability import FreqDist\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def scrape_article(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    text = \" \".join(p.get_text(strip=True) for p in soup.find_all('p'))  # Combine all paragraph text\n",
        "    return text\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [\n",
        "        token.lower() for token in tokens\n",
        "        if token.lower() not in stop_words and token not in string.punctuation\n",
        "    ]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    stemmed_tokens =  filtered_tokens\n",
        "    return stemmed_tokens\n",
        "\n",
        "\n",
        "def get_main_keywords(tokens, n=10):\n",
        "    freq_dist = FreqDist(tokens)\n",
        "    main_keywords = freq_dist.most_common(n)\n",
        "    return main_keywords\n",
        "\n",
        "\n",
        "def keywords_to_dataframe(keywords):\n",
        "    df = pd.DataFrame(keywords, columns=['keyword', 'frequency'])\n",
        "    return df\n",
        "\n",
        "def suggest_topic(keywords):\n",
        "    # Basic topic suggestion based on keywords (can be improved)\n",
        "    topic = \", \".join([keyword for keyword, freq in keywords[:3]])\n",
        "    return f\"Suggested Topic: {topic}\"\n",
        "\n",
        "def analyze_article(url):\n",
        "    text = scrape_article(url)\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    main_keywords = get_main_keywords(preprocessed_text)\n",
        "    keywords_df = keywords_to_dataframe(main_keywords)\n",
        "    suggested_topic = suggest_topic(main_keywords) #Generate topic suggestion\n",
        "    return keywords_df, suggested_topic\n",
        "\n",
        "url = 'https://warwick.ac.uk/fac/sci/wmg/news-and-events/news/wmgnews/celebrating_excellence_wmg'\n",
        "keywords_df, suggested_topic = analyze_article(url)\n",
        "print(keywords_df)\n",
        "suggested_topic"
      ],
      "metadata": {
        "id": "dsYGaCgDv8_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}